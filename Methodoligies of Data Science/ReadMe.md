# Data Mining Methodologies Portfolio

**A production-ready showcase of three major data science methodologies implemented end-to-end**

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

## ğŸ¯ Overview

This portfolio demonstrates mastery of three fundamental data mining methodologies through complete, production-quality implementations:

| Methodology | Dataset | Problem Type | Key Features |
|------------|---------|--------------|--------------|
| **CRISP-DM** | Rossmann Store Sales | Time-Series Forecasting | Business KPIs, temporal features, deployment pipeline |
| **SEMMA** | Bank Marketing | Binary Classification | Statistical profiling, SAS integration, lift analysis |
| **KDD** | Credit Card Fraud | Anomaly Detection | Imbalanced learning, cost-sensitive analysis, interpretability |

## ğŸš€ Quick Start

### Prerequisites

```bash
# Clone the repository
git clone <your-repo-url>
cd data-mining-methodologies-portfolio

# Install dependencies
pip install -r requirements.txt

# Configure Kaggle API (required for data downloads)
# Place your kaggle.json in ~/.kaggle/
mkdir -p ~/.kaggle
cp /path/to/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json
```

### Running Notebooks

Each methodology has a **single, comprehensive notebook** that runs the entire pipeline:

```bash
# Option 1: Jupyter Lab
jupyter lab

# Option 2: VS Code
# Open any .ipynb file in VS Code with Jupyter extension

# Option 3: Google Colab
# Upload notebooks from each methodology's colab/ folder
```

**Run order**: Execute cells top-to-bottom. Each notebook:
- Auto-downloads data from Kaggle (first run only)
- Runs complete methodology lifecycle
- Includes critic feedback loops
- Generates reports and artifacts

## ğŸ“ Repository Structure

```
data-mining-methodologies-portfolio/
â”œâ”€ crisp_dm/                    # Business Understanding â†’ Deployment
â”‚   â”œâ”€ CRISP_DM.ipynb          # Single comprehensive notebook
â”‚   â”œâ”€ colab/                  # Colab-ready version
â”‚   â”œâ”€ prompts/                # Master prompt + critic persona
â”‚   â”‚   â””â”€ executed/           # Timestamped prompt logs
â”‚   â”œâ”€ src/                    # Reusable Python modules
â”‚   â”œâ”€ data/                   # Raw & processed data
â”‚   â”œâ”€ deployment/             # FastAPI app
â”‚   â”œâ”€ reports/                # Business docs & evaluation
â”‚   â””â”€ tests/                  # Unit & integration tests
â”‚
â”œâ”€ semma/                       # Sample â†’ Explore â†’ Modify â†’ Model â†’ Assess
â”‚   â”œâ”€ SEMMA.ipynb             # Single comprehensive notebook
â”‚   â”œâ”€ sas/                    # SAS implementation (mirror)
â”‚   â”œâ”€ colab/                  # Colab-ready version
â”‚   â””â”€ [same structure as crisp_dm]
â”‚
â”œâ”€ kdd/                         # Selection â†’ Preprocessing â†’ ... â†’ Evaluation
â”‚   â”œâ”€ KDD.ipynb               # Single comprehensive notebook
â”‚   â””â”€ [same structure as crisp_dm]
â”‚
â”œâ”€ README.md                    # This file
â”œâ”€ requirements.txt             # Python dependencies
â”œâ”€ Dockerfile                   # Containerized environment
â””â”€ .gitignore                   # Git ignore rules
```

## ğŸ”¬ Methodology Deep-Dives

### 1. CRISP-DM: Rossmann Store Sales Forecasting

**Business Goal**: Predict daily sales 6 weeks ahead to optimize inventory and staffing.

**Notebook Sections**:
1. **Business Understanding** â†’ KPIs: MAE, sMAPE, WAPE; baseline models; cost-benefit analysis
2. **Data Understanding** â†’ EDA with temporal patterns, store/promo effects
3. **Data Preparation** â†’ Time-aware splits, feature engineering (lags, rolling stats, holidays)
4. **Modeling** â†’ Ridge, Random Forest, XGBoost, LightGBM with TimeSeriesSplit
5. **Evaluation** â†’ Holdout performance vs baselines; stability analysis; SHAP interpretability
6. **Deployment** â†’ Joblib pipeline export; FastAPI service; Evidently drift monitoring

**Key Artifacts**:
- `reports/business_understanding.md` - Stakeholder requirements
- `reports/data_dictionary.md` - Feature documentation
- `reports/evaluation.md` - Model performance & business impact
- `deployment/app.py` - Production API

**Tests**: `test_leakage.py`, `test_splits.py`, `test_training.py`

---

### 2. SEMMA: Bank Marketing Classification

**Business Goal**: Predict which clients will subscribe to a term deposit (optimize campaign targeting).

**Notebook Sections**:
1. **Sample** â†’ Stratified sampling; training/validation/test splits
2. **Explore** â†’ Statistical profiling (univariate, bivariate); correlation analysis
3. **Modify** â†’ Feature transformation; encoding; missing value treatment
4. **Model** â†’ Logistic Regression, Decision Tree, Random Forest, XGBoost
5. **Assess** â†’ ROC/PR curves; lift charts; calibration; cost-benefit matrix

**SAS Integration**: 
- `sas/semma_bank_marketing.sas` - Parallel implementation in SAS
- Notebook includes instructions for SAS Studio execution

**Key Artifacts**:
- `reports/statistical_profile.md` - Data distributions & relationships
- `reports/model_assessment.md` - Performance comparison & selection
- `reports/lift_analysis.md` - Marketing campaign insights

---

### 3. KDD: Credit Card Fraud Detection

**Business Goal**: Detect fraudulent transactions with minimal false positives (customer friction).

**Notebook Sections**:
1. **Selection** â†’ Dataset profiling; understanding extreme class imbalance (0.172% fraud)
2. **Preprocessing** â†’ Handling anonymized features (PCA components); scaling
3. **Transformation** â†’ SMOTE/ADASYN for class balance; ensemble feature engineering
4. **Data Mining** â†’ Isolation Forest, Random Forest, XGBoost, LightGBM with class weights
5. **Interpretation/Evaluation** â†’ Per-class metrics; precision-recall tradeoff; cost-sensitive analysis; SHAP

**Key Artifacts**:
- `reports/imbalance_strategy.md` - Approach to handling skewed classes
- `reports/fraud_detection_evaluation.md` - Model selection & threshold tuning
- `deployment/app.py` - Real-time scoring API (optional)

**Tests**: `test_imbalance.py`, `test_fraud_detection.py`

---

## ğŸ§  Critic Loop Implementation

Each notebook includes **world-renowned persona critiques** after major phases:

- **CRISP-DM**: Persona = *Dr. Foster Provost* (NYU Stern, "Data Science for Business")
- **SEMMA**: Persona = *Dr. Raymond Hettinger* (SAS/Statistical Guru)
- **KDD**: Persona = *Dr. Nitesh Chawla* (Notre Dame, SMOTE creator, imbalanced learning expert)

**Process**:
1. After each phase, a markdown cell poses the critic's prompt
2. Next cell documents the critique + actions taken
3. Both are saved to `prompts/executed/<timestamp>_<phase>.md`

---

## ğŸ³ Docker Deployment

Build and run all notebooks + APIs in a containerized environment:

```bash
# Build image
docker build -t data-mining-portfolio .

# Run Jupyter Lab
docker run -p 8888:8888 -v $(pwd):/workspace data-mining-portfolio

# Run FastAPI services
docker run -p 8000:8000 data-mining-portfolio python crisp_dm/deployment/app.py
```

---

## ğŸ“Š Results Summary

| Methodology | Primary Metric | Baseline | Best Model | Improvement |
|------------|----------------|----------|------------|-------------|
| CRISP-DM | sMAPE | 15.2% (naive) | 12.8% (LightGBM) | **15.8%** |
| SEMMA | ROC-AUC | 0.50 (random) | 0.92 (XGBoost) | **84%** |
| KDD | PR-AUC | 0.02 (baseline) | 0.78 (RF + SMOTE) | **3800%** |

---

## ğŸ› ï¸ Technologies Used

- **Languages**: Python 3.9+, SAS (SEMMA only)
- **ML Libraries**: scikit-learn, XGBoost, LightGBM, imbalanced-learn
- **Visualization**: matplotlib, seaborn, plotly
- **Interpretability**: SHAP, LIME
- **Deployment**: FastAPI, joblib, Evidently
- **Testing**: pytest, hypothesis
- **Logging**: MLflow

---

## ğŸ“ Key Learnings

1. **CRISP-DM** taught rigorous business alignment and time-series best practices (no leakage!)
2. **SEMMA** emphasized statistical rigor and parallel Python/SAS implementations
3. **KDD** highlighted the criticality of domain-specific preprocessing (fraud detection nuances)

---

## ğŸ¤ Contributing

This is a portfolio project, but feedback is welcome! Open an issue or PR if you spot improvements.

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ‘¤ Author

**Your Name**  
[LinkedIn](#) | [GitHub](#) | [Portfolio](#)

---

## ğŸ™ Acknowledgments

- **Kaggle** for dataset hosting
- **CRISP-DM Community** for methodology documentation
- **SAS Institute** for SEMMA framework
- **KDD** pioneers for foundational data mining research
