## Clustering Methods: K-Means vs GMM vs DBSCAN

This project tries multiple clustering methods on the same standardized features (Age, Income, Spending Score):

### 1) K-Means (partitioning, distance-based)
**Idea:** Assign points to the nearest of `k` centroids; update centroids until stable.  
**Pros**
- Fast, simple, scalable.
- Works well when clusters are **compact and spherical** in standardized space.
- Easy to explain and deploy.
**Cons**
- You must choose `k`.
- Struggles with non-spherical shapes and unequal cluster sizes/densities.
- Sensitive to outliers.

**Use when:** you expect roughly globular clusters and want speed + interpretability.  
**How we pick `k`:** **Elbow** (inertia ↓) + **Silhouette** (↑). We default to the `k` with the highest Silhouette, unless the Elbow clearly suggests a simpler `k`.

---

### 2) Gaussian Mixture Model (GMM) (probabilistic, soft assignments)
**Idea:** Assume data is generated by a mixture of `k` Gaussians. Each point gets **probabilities** of belonging to each cluster.  
**Pros**
- Captures **elliptical** clusters; more flexible than K-Means.
- Soft assignments (responsibilities) can reflect uncertainty.
- Model selection via AIC/BIC (optional).
**Cons**
- You must choose `k` (or compare AIC/BIC over many `k`).
- Can be sensitive to initialization and local optima.
- Not ideal for very non-Gaussian shapes or heavy outliers.

**Use when:** clusters are **elliptical** / overlapping and you want **soft** cluster membership.

---

### 3) DBSCAN (density-based, no k)
**Idea:** Groups points that are densely packed; labels sparse points as **noise** (`-1`).  
**Pros**
- **No need to choose `k`**.
- Finds **arbitrary shapes** (not limited to spheres/ellipses).
- Naturally detects outliers as noise.
**Cons**
- Need to set `eps` and `min_samples`; sensitive to these.
- Can merge/split clusters oddly when densities vary.
- Doesn’t scale as easily to very high dimensions without tuning.

**Use when:** you expect **arbitrary shapes** / **varying densities** and want **outlier detection**. Good for exploratory analysis.

---

## How We Compare Methods

- **Silhouette Score** (↑ better): Measures separation/compactness using current labels.
- **Adjusted Rand Index (ARI)** (↑ better): Compares two labelings (we compare GMM/DBSCAN labels **against K-Means** as a reference).  
  *Note:* ARI = 1.0 when comparing K-Means with itself by definition.

We also provide:
- **PCA 2D plots** for visual intuition.
- **Cluster centers in original units** (for K-Means): easy to interpret (e.g., average age, income, score per cluster).
- **Cluster profile tables** (means per cluster): handy for business summaries.

---

## Choosing the Right Method (Quick Guide)

- **Start with K-Means**: fast baseline, good if standardized features look roughly globular.
- **If clusters look stretched or overlapping**: try **GMM** (elliptical blobs, soft membership).
- **If you see weird shapes, chains, or many outliers**: try **DBSCAN** (detects non-convex clusters + noise).
- **If `k` is unknown**: DBSCAN avoids choosing `k`, or use Elbow/Silhouette (K-Means) or AIC/BIC (GMM) to guide selection.

---

## Practical Tips

- **Always standardize** numeric features before distance-based clustering.
- **Check stability**: re-run with different random seeds (we report mean±std ARI vs. the K-Means run).
- **Inspect scatter/PCA plots**: numbers + visuals together give better intuition.
- **Mind scale & units**: report centers in **original units** for business folks.
- **Document parameters**: `k`, `eps`, `min_samples`, and how you picked them.
- **Outliers**: If many points become noise in DBSCAN, reduce `eps` percentile or adjust `min_samples`.

---

## Reporting (What to include)

- **Method & params**: e.g., “K-Means, k=5, standardized features”.
- **Metrics**: Silhouette (per method) and ARI (vs reference).
- **Plots**: Elbow, Silhouette vs k; PCA 2D cluster scatter.
- **Interpretation**: Cluster sizes and **profiles** (means per feature, original units).
- **Actionable insights**: e.g., “Cluster 3 = young, high-spend; target with loyalty program”.
