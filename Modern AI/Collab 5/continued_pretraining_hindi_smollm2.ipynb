{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4d86bf",
   "metadata": {},
   "source": [
    "# Continued Pre-training with SmolLM2-135M using Unsloth\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **Continued Pre-training** to teach SmolLM2-135M a new language (Hindi) using Unsloth.ai.\n",
    "\n",
    "### What is Continued Pre-training?\n",
    "- Continued pre-training extends a model's knowledge to new domains/languages\n",
    "- Different from fine-tuning (which adapts existing knowledge)\n",
    "- Teaches fundamentally new capabilities\n",
    "- Uses next-token prediction on raw text\n",
    "\n",
    "### Model Details\n",
    "- **Model**: SmolLM2-135M-Instruct\n",
    "- **Method**: Continued pre-training with LoRA (r=16)\n",
    "- **Task**: Learn Hindi language\n",
    "- **Dataset**: Hindi text corpus\n",
    "\n",
    "### Key Concepts\n",
    "- **Pre-training**: Learning language fundamentals\n",
    "- **Next-token prediction**: Predicting what comes next\n",
    "- **Language adaptation**: Teaching new language\n",
    "- **Domain adaptation**: Teaching new knowledge areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4373271",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth\n",
    "!pip install -q unsloth\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e285ec",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b03d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(\"âœ“ Ready for continued pre-training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5566b7",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2120dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 512\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Method: Continued Pre-training\")\n",
    "print(f\"  Task: Teaching Hindi language\")\n",
    "print(f\"  Current: Model mainly knows English\")\n",
    "print(f\"  Goal: Extend to Hindi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc20e4",
   "metadata": {},
   "source": [
    "## Step 4: Load the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7948a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Base model loaded successfully!\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4600dbe",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Model for Continued Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA for efficient continued pre-training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    \"embed_tokens\", \"lm_head\"],  # Include embeddings for new language\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],  # Save embedding updates\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model prepared for continued pre-training!\")\n",
    "print(\"  Including embeddings for new language tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b892b4",
   "metadata": {},
   "source": [
    "## Step 6: Create Hindi Text Dataset\n",
    "\n",
    "For continued pre-training, we need raw text in the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small Hindi text corpus\n",
    "# In real scenarios, use large Hindi datasets from HuggingFace\n",
    "hindi_texts = [\n",
    "    \"à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆà¥¤ à¤®à¥ˆà¤‚ à¤­à¤¾à¤°à¤¤ à¤¸à¥‡ à¤¹à¥‚à¤‚à¥¤\",\n",
    "    \"à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤ à¤¸à¥‚à¤°à¤œ à¤šà¤®à¤• à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤®à¥à¤à¥‡ à¤•à¤¿à¤¤à¤¾à¤¬à¥‡à¤‚ à¤ªà¤¢à¤¼à¤¨à¤¾ à¤¬à¤¹à¥à¤¤ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤µà¤¿à¤œà¥à¤žà¤¾à¤¨ à¤à¤• à¤°à¥‹à¤šà¤• à¤µà¤¿à¤·à¤¯ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¤¾ à¤à¤• à¤¹à¤¿à¤¸à¥à¤¸à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤ªà¤¾à¤¯à¤¥à¤¨ à¤à¤• à¤²à¥‹à¤•à¤ªà¥à¤°à¤¿à¤¯ à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤®à¤¿à¤‚à¤— à¤­à¤¾à¤·à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤®à¥ˆà¤‚ à¤°à¥‹à¤œ à¤¸à¥à¤¬à¤¹ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤°à¤¤à¤¾ à¤¹à¥‚à¤‚à¥¤\",\n",
    "    \"à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¬à¤¹à¥à¤¤ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤•à¤ˆ à¤­à¤¾à¤·à¤¾à¤à¤‚ à¤¬à¥‹à¤²à¥€ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆà¤‚à¥¤\",\n",
    "    \"à¤¤à¤•à¤¨à¥€à¤• à¤¨à¥‡ à¤¹à¤®à¤¾à¤°à¥€ à¤œà¤¿à¤‚à¤¦à¤—à¥€ à¤¬à¤¦à¤² à¤¦à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤¡à¥€à¤ª à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤¨à¥à¤¯à¥‚à¤°à¤² à¤¨à¥‡à¤Ÿà¤µà¤°à¥à¤• à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤®à¥à¤à¥‡ à¤¸à¤‚à¤—à¥€à¤¤ à¤¸à¥à¤¨à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤µà¤¿à¤œà¥à¤žà¤¾à¤¨ à¤¹à¤®à¥‡à¤‚ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤•à¥‹à¤¡à¤¿à¤‚à¤— à¤¸à¥€à¤–à¤¨à¤¾ à¤†à¤œà¤•à¤² à¤¬à¤¹à¥à¤¤ à¤œà¤°à¥‚à¤°à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤¹à¤¿à¤‚à¤¦à¥€ à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¥€ à¤­à¤¾à¤·à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "]\n",
    "\n",
    "# Add bilingual examples to help bridging\n",
    "bilingual_texts = [\n",
    "    \"Hello à¤•à¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤•à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\",\n",
    "    \"Computer à¤•à¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤•à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\",\n",
    "    \"Thank you à¤•à¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦ à¤•à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\",\n",
    "    \"Python programming in Hindi: à¤ªà¤¾à¤¯à¤¥à¤¨ à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤®à¤¿à¤‚à¤—\",\n",
    "    \"Machine Learning in Hindi: à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤—\",\n",
    "]\n",
    "\n",
    "# Combine and repeat to get 100+ examples\n",
    "all_texts = hindi_texts + bilingual_texts\n",
    "extended_texts = []\n",
    "for _ in range(6):  # Repeat to get ~120 examples\n",
    "    extended_texts.extend(all_texts)\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_dict({\"text\": extended_texts})\n",
    "\n",
    "print(f\"âœ“ Hindi text dataset created: {len(dataset)} examples\")\n",
    "print(\"\\nSample texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. {dataset[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b20c06",
   "metadata": {},
   "source": [
    "## Step 7: Format Dataset for Pre-training\n",
    "\n",
    "For pre-training, we use raw text with EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965179",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_pretraining(example):\n",
    "    \"\"\"Format text for continued pre-training.\"\"\"\n",
    "    # For pre-training, we just add EOS token\n",
    "    return {\"text\": example[\"text\"] + EOS_TOKEN}\n",
    "\n",
    "dataset = dataset.map(format_pretraining)\n",
    "\n",
    "print(\"âœ“ Dataset formatted for pre-training!\")\n",
    "print(\"\\nFormatted example:\")\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d87ced",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training Arguments\n",
    "\n",
    "Pre-training typically uses different hyperparameters than fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79365f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continued pre-training configuration\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,  # More warmup for pre-training\n",
    "    max_steps=100,     # More steps for language learning\n",
    "    learning_rate=3e-4,  # Slightly higher for pre-training\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine schedule for pre-training\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs_pretrain\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Pre-training arguments configured!\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Note: Slightly different from fine-tuning config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06651ebe",
   "metadata": {},
   "source": [
    "## Step 9: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f40cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer for continued pre-training\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Continued pre-training trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623df28",
   "metadata": {},
   "source": [
    "## Step 10: Continue Pre-training on Hindi\n",
    "\n",
    "Train the model to understand and generate Hindi text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting continued pre-training on Hindi...\")\n",
    "print(\"Teaching the model a new language!\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Continued pre-training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"\\nModel now has Hindi language capability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec75d3",
   "metadata": {},
   "source": [
    "## Step 11: Test Hindi Generation\n",
    "\n",
    "Test if the model can understand and generate Hindi text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a70c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test with Hindi prompt\n",
    "test_prompt = \"à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤—\"\n",
    "\n",
    "print(\"Test Prompt (Hindi):\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"Model's Hindi Generation:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Note: Model should continue in Hindi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da8495",
   "metadata": {},
   "source": [
    "## Step 12: More Hindi Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hindi(prompt):\n",
    "    \"\"\"Test Hindi generation.\"\"\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nGenerated: {response}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "print(\"Testing Hindi generation with various prompts...\\n\")\n",
    "\n",
    "test_hindi(\"à¤¨à¤®à¤¸à¥à¤¤à¥‡,\")\n",
    "test_hindi(\"à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤°\")\n",
    "test_hindi(\"à¤ªà¤¾à¤¯à¤¥à¤¨ à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤®à¤¿à¤‚à¤—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6465379",
   "metadata": {},
   "source": [
    "## Step 13: Test Bilingual Capability\n",
    "\n",
    "Test if model retained English while learning Hindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108aea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing bilingual capability (English + Hindi)...\\n\")\n",
    "\n",
    "# Test English (should still work)\n",
    "english_prompt = \"Machine learning is\"\n",
    "inputs = tokenizer([english_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)\n",
    "english_response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"English Test:\")\n",
    "print(f\"Prompt: {english_prompt}\")\n",
    "print(f\"Generated: {english_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test Hindi\n",
    "hindi_prompt = \"à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤—\"\n",
    "inputs = tokenizer([hindi_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)\n",
    "hindi_response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Hindi Test:\")\n",
    "print(f\"Prompt: {hindi_prompt}\")\n",
    "print(f\"Generated: {hindi_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Model should work in both languages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089079c5",
   "metadata": {},
   "source": [
    "## Step 14: Save the Multilingual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the multilingual model\n",
    "model.save_pretrained(\"smollm2_135m_hindi\")\n",
    "tokenizer.save_pretrained(\"smollm2_135m_hindi\")\n",
    "\n",
    "print(\"âœ“ Multilingual model saved to 'smollm2_135m_hindi' directory\")\n",
    "print(\"\\nThe model now:\")\n",
    "print(\"  âœ… Understands Hindi\")\n",
    "print(\"  âœ… Generates Hindi text\")\n",
    "print(\"  âœ… Retains English capability\")\n",
    "print(\"  âœ… Is bilingual (English + Hindi)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c21e6",
   "metadata": {},
   "source": [
    "## Step 15: Understanding Continued Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNDERSTANDING CONTINUED PRE-TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“š What is Continued Pre-training?\")\n",
    "print(\"  Continued pre-training extends a model's knowledge:\")\n",
    "print(\"  â€¢ Teaches NEW languages (like Hindi)\")\n",
    "print(\"  â€¢ Teaches NEW domains (like medical, legal)\")\n",
    "print(\"  â€¢ Uses raw text (no instruction format)\")\n",
    "print(\"  â€¢ Next-token prediction objective\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Difference from Fine-tuning:\")\n",
    "print(\"  Fine-tuning:\")\n",
    "print(\"    â€¢ Adapts EXISTING knowledge\")\n",
    "print(\"    â€¢ Uses instruction-output pairs\")\n",
    "print(\"    â€¢ Short training (60-100 steps)\")\n",
    "print(\"    â€¢ For task adaptation\")\n",
    "print(\"  \")\n",
    "print(\"  Continued Pre-training:\")\n",
    "print(\"    â€¢ Teaches NEW knowledge\")\n",
    "print(\"    â€¢ Uses raw text corpus\")\n",
    "print(\"    â€¢ Longer training (1000s of steps ideally)\")\n",
    "print(\"    â€¢ For capability expansion\")\n",
    "\n",
    "print(\"\\nðŸ“Š This Notebook:\")\n",
    "print(\"  â€¢ Taught Hindi to an English model\")\n",
    "print(\"  â€¢ Used 100+ Hindi text examples\")\n",
    "print(\"  â€¢ Trained for 100 steps (minimal but demonstrative)\")\n",
    "print(\"  â€¢ Model now bilingual!\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Real-world Applications:\")\n",
    "print(\"  â€¢ Multilingual models (add languages)\")\n",
    "print(\"  â€¢ Domain-specific models (medical, legal, code)\")\n",
    "print(\"  â€¢ Time-updated models (learn new events/facts)\")\n",
    "print(\"  â€¢ Custom knowledge bases\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Requirements:\")\n",
    "print(\"  â€¢ Large corpus in target domain/language\")\n",
    "print(\"  â€¢ Sufficient training time\")\n",
    "print(\"  â€¢ Monitor for catastrophic forgetting\")\n",
    "print(\"  â€¢ Balance old vs new knowledge\")\n",
    "\n",
    "print(\"\\nâœ… What We Learned:\")\n",
    "print(\"  â€¢ How to adapt models to new languages\")\n",
    "print(\"  â€¢ Difference between pre-training and fine-tuning\")\n",
    "print(\"  â€¢ Bilingual capability development\")\n",
    "print(\"  â€¢ Knowledge expansion techniques\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083141d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did:\n",
    "1. âœ… Loaded English-based SmolLM2-135M\n",
    "2. âœ… Created Hindi text corpus\n",
    "3. âœ… Continued pre-training on Hindi\n",
    "4. âœ… Tested Hindi generation\n",
    "5. âœ… Verified bilingual capability\n",
    "6. âœ… Saved the multilingual model\n",
    "\n",
    "### Key Concepts:\n",
    "- **Continued Pre-training**: Teaching new knowledge/languages\n",
    "- **Language Adaptation**: Extending to new languages\n",
    "- **Knowledge Retention**: Keeping existing capabilities\n",
    "- **Bilingual Models**: Supporting multiple languages\n",
    "\n",
    "### Complete Journey Across All Colabs:\n",
    "- **Colab 1**: Full fine-tuning (high rank, instruction following)\n",
    "- **Colab 2**: LoRA fine-tuning (efficient, instruction following)\n",
    "- **Colab 3**: DPO (preference learning, alignment)\n",
    "- **Colab 4**: GRPO (reasoning, chain-of-thought)\n",
    "- **Colab 5**: Continued pre-training (new language) â­\n",
    "\n",
    "### When to Use Continued Pre-training:\n",
    "- âœ… Need to add new language\n",
    "- âœ… Want domain-specific model\n",
    "- âœ… Have large corpus of raw text\n",
    "- âœ… Need fundamentally new knowledge\n",
    "\n",
    "### Important Notes:\n",
    "- This demo uses minimal data (~120 examples)\n",
    "- Real language learning needs 1000s-millions of examples\n",
    "- Training for 100 steps is demonstrative\n",
    "- Production would need 10,000+ steps\n",
    "\n",
    "### Next Steps for Production:\n",
    "1. Use larger Hindi corpus (e.g., Oscar, CC100)\n",
    "2. Train for many more steps (10K-100K)\n",
    "3. Monitor perplexity on Hindi and English\n",
    "4. Use learning rate schedule carefully\n",
    "5. Validate on both languages regularly\n",
    "\n",
    "### Resources:\n",
    "- Continued Pre-training Guide: https://docs.unsloth.ai/basics/continued-pretraining\n",
    "- Unsloth Documentation: https://docs.unsloth.ai/\n",
    "- Hindi Datasets: Oscar, CC100, IndicCorp\n",
    "\n",
    "## ðŸŽ‰ Assignment Complete!\n",
    "\n",
    "You now have all 5 Colab notebooks:\n",
    "1. âœ… Full Fine-tuning\n",
    "2. âœ… LoRA Fine-tuning\n",
    "3. âœ… DPO Preference Learning\n",
    "4. âœ… GRPO Reasoning Model\n",
    "5. âœ… Continued Pre-training\n",
    "\n",
    "Record your YouTube videos explaining each approach! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
