# Colab 5: Continued Pre-training for Hindi Language

## ğŸ¯ Overview

This notebook demonstrates **Continued Pre-training** to teach SmolLM2-135M a new language (Hindi). Unlike fine-tuning which adapts models to tasks, continued pre-training expands the model's fundamental knowledge base, enabling it to understand and generate text in languages it wasn't originally trained on.

## ğŸŒŸ What is Continued Pre-training?

**Continued Pre-training** extends a model's knowledge by training on raw text using next-token prediction:
- **Adds new languages**: Teach models Hindi, Arabic, Chinese, etc.
- **Expands domains**: Medical, legal, technical knowledge
- **Updates knowledge**: Recent events, new research
- **Maintains existing capabilities**: Doesn't forget original knowledge (with proper technique)

## ğŸŒ Why This Matters

### The Language Gap
- Most AI models are English-centric
- Billions of people speak other languages
- Regional languages are underserved
- Cultural context is often lost

### Making AI Accessible
Continued pre-training enables:
- ğŸŒ **Regional language support**: Hindi, Tamil, Bengali, etc.
- ğŸ›ï¸ **Government services**: Local language AI
- ğŸ“± **Mobile apps**: Underserved markets
- ğŸ“š **Education**: Learning in native languages
- ğŸ­ **Cultural preservation**: Endangered languages

## ğŸ”¬ Method Comparison

| Aspect | Fine-tuning (Colab 1-4) | Continued Pre-training (Colab 5) |
|--------|-------------------------|----------------------------------|
| **Purpose** | Task adaptation | Knowledge expansion |
| **Input** | Instruction-response pairs | Raw text |
| **Training** | Supervised learning | Next-token prediction |
| **Data Size** | 100s-1000s examples | Millions of tokens |
| **Goal** | Learn task | Learn language/domain |
| **Modules** | Query, key, value | Embeddings + all layers |
| **Use Case** | After pre-training | Extend base knowledge |

## ğŸ“š Dataset Design

### Format
Raw text with bilingual content to prevent catastrophic forgetting:

```python
{
    "text": "Hindi sentence. (English translation.) More Hindi..."
}
```

### Example
```python
{
    "text": "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥ˆà¤‚ à¤à¤• à¤›à¤¾à¤¤à¥à¤° à¤¹à¥‚à¤‚à¥¤ (Hello, I am a student.) à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤ (I like learning Hindi.)"
}
```

### Why Bilingual?
- âœ… **Prevents forgetting**: Maintains English capability
- âœ… **Translation bridges**: Learns language connections
- âœ… **Efficient learning**: Leverages existing knowledge
- âœ… **Better alignment**: Maps concepts across languages
- âœ… **Code-switching**: Handles mixed-language input

### Dataset Coverage
Our 120 examples include:
- Basic greetings and introductions
- Numbers and counting (1-100)
- Daily activities and routines
- Family and relationships
- Food and cultural terms
- Common phrases and questions
- Colors, days, months
- Simple conversations

## ğŸš€ Quick Start

### Prerequisites
- Google Colab account (free tier works!)
- GPU runtime (T4 recommended)
- ~12-15 minutes for training

### Steps to Run

1. **Upload to Google Colab**
   ```
   - Go to https://colab.research.google.com/
   - File â†’ Upload notebook
   - Select `continued_pretraining_hindi_smollm2.ipynb`
   ```

2. **Enable GPU**
   ```
   - Runtime â†’ Change runtime type
   - Hardware accelerator: T4 GPU
   - Save
   ```

3. **Run All Cells**
   ```
   - Runtime â†’ Run all (Ctrl/Cmd + F9)
   - Wait ~12-15 minutes
   - Watch Hindi learning happen!
   ```

4. **Test Bilingual Capability**
   ```
   - Try Hindi prompts
   - Test Englishâ†’Hindi
   - Experiment with code-switching
   ```

## âš™ï¸ Configuration

### Model Configuration
```python
model_name = "unsloth/SmolLM2-135M-Instruct"
max_seq_length = 512
load_in_4bit = True
```

### LoRA Configuration (Critical!)
```python
r = 16
lora_alpha = 16
lora_dropout = 0
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
modules_to_save = ["embed_tokens", "lm_head"]  # CRITICAL!
```

### Why `modules_to_save`?

**embed_tokens** (Input Embeddings):
- Converts tokens â†’ vectors
- Must adapt to Hindi tokens
- If frozen, can't understand Hindi
- **Must be trainable!**

**lm_head** (Output Layer):
- Predicts next tokens
- Must generate Hindi tokens
- If frozen, can't produce Hindi
- **Must be trainable!**

Without these, language learning fails!

### Training Arguments
```python
per_device_train_batch_size = 2
gradient_accumulation_steps = 4
learning_rate = 2e-4           # Higher than fine-tuning
max_steps = 100                # More steps for language
warmup_steps = 10
```

**Why more steps?**
- Language learning needs more exposure
- Need to see patterns repeatedly
- Building fundamental knowledge, not task adaptation

## ğŸ“ˆ Expected Results

### Training Metrics
- **Initial Loss**: ~3.0-3.5 (model doesn't know Hindi)
- **Final Loss**: ~1.5-2.0 (learning patterns)
- **Training Time**: 12-15 minutes on T4 GPU
- **Trainable Params**: ~4M LoRA + embeddings + lm_head

### Language Capability

**Before Continued Pre-training**:
```
Input: "à¤¨à¤®à¤¸à¥à¤¤à¥‡"
Output: [gibberish or English fallback]
```

**After Continued Pre-training**:
```
Input: "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥ˆà¤‚"
Output: "à¤à¤• à¤›à¤¾à¤¤à¥à¤° à¤¹à¥‚à¤‚" (I am a student)
```

### What to Expect
- âœ… Generate basic Hindi sentences
- âœ… Understand common Hindi phrases
- âœ… Translate simple Englishâ†’Hindi
- âœ… Handle bilingual/code-switched input
- âš ï¸ Not fluent (needs more data)
- âš ï¸ Limited vocabulary (120 examples)

## ğŸ§ª Testing Examples

### Test 1: Hindi Generation
```python
prompt = "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥ˆà¤‚"
```
**Expected**: Continues in Hindi (e.g., "à¤à¤• à¤›à¤¾à¤¤à¥à¤° à¤¹à¥‚à¤‚")

### Test 2: English to Hindi
```python
prompt = "Translate to Hindi: Hello, how are you?"
```
**Expected**: "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?"

### Test 3: Code-Switching
```python
prompt = "Hello, à¤®à¥ˆà¤‚"
```
**Expected**: Handles mixed language appropriately

### Test 4: Hindi Question
```python
prompt = "à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?"
```
**Expected**: Appropriate Hindi response

## ğŸ“Š Performance Assessment

| Capability | Before | After (100 steps) | Production (1M steps) |
|------------|--------|-------------------|----------------------|
| Hindi Generation | 0% | 40-60% | 90%+ |
| Hindi Understanding | 0% | 50-70% | 95%+ |
| Translation | 0% | 30-50% | 85%+ |
| Code-Switching | 0% | 40-60% | 90%+ |
| English Retention | 100% | 85-95% | 95%+ |

## ğŸ’¾ Model Saving Options

### 1. Save LoRA Adapters + Special Modules
```python
model.save_pretrained("smollm2_hindi")
tokenizer.save_pretrained("smollm2_hindi")
```
- **Size**: ~20-30MB (includes embeddings)
- **Use**: Load with base model

### 2. Save Merged Bilingual Model
```python
model.save_pretrained_merged(
    "smollm2_hindi_merged",
    tokenizer,
    save_method="merged_16bit"
)
```
- **Size**: ~270MB
- **Use**: Standalone bilingual model

### 3. Export to GGUF
```python
model.save_pretrained_gguf(
    "smollm2_hindi",
    tokenizer,
    quantization_method="q4_k_m"
)
```
- **Size**: ~80-100MB
- **Use**: Ollama for local bilingual inference

## ğŸ¯ When to Use Continued Pre-training

### âœ… Use Continued Pre-training When:
- Teaching new languages
- Adding domain knowledge (medical, legal)
- Updating with recent information
- Adapting to specialized vocabulary
- Creating multilingual models
- Supporting underserved languages

### âŒ Don't Use Continued Pre-training When:
- Just need task adaptation (use fine-tuning)
- Teaching instruction-following (use SFT)
- Aligning preferences (use DPO)
- Adding reasoning (use GRPO)
- Have limited data (<100K tokens)

## ğŸ—ï¸ Production Scaling

### Data Requirements

**Proof of Concept** (this notebook):
- Size: 120 bilingual examples (~20K tokens)
- Time: 12-15 minutes
- Result: Basic capability demonstration

**Research Quality**:
- Size: 10-100 million tokens
- Time: 10-100 GPU hours
- Result: Functional bilingual model

**Production Quality**:
- Size: 1-10 billion tokens
- Time: 1000+ GPU hours
- Result: Fluent multilingual model

### Data Sources for Hindi
- **OSCAR**: Large-scale web crawl
- **Wikipedia**: High-quality Hindi content
- **News corpora**: Current, clean text
- **Books**: Literary Hindi
- **Subtitles**: Conversational Hindi
- **Government docs**: Formal Hindi

### Training at Scale
```python
# Production configuration
max_steps = 100000              # Much longer
per_device_train_batch_size = 8 # Larger batches
gradient_accumulation_steps = 8 # Effective batch = 64
learning_rate = 1e-4            # May need tuning
eval_steps = 1000               # Evaluate periodically
```

## ğŸ” Preventing Catastrophic Forgetting

### The Problem
When learning Hindi, model might forget English!

### Our Solutions

1. **Bilingual Training Data**
   - Mix Hindi and English in each example
   - Maintains both languages

2. **Translation Pairs**
   - Shows language equivalence
   - Reinforces both languages

3. **Balanced Curriculum**
   - Don't overwhelm with only Hindi
   - Keep English exposure

4. **Regular Evaluation**
   - Test English capability
   - Stop if degradation detected

### Monitoring
```python
# Check English retention
english_prompts = ["What is AI?", "Hello, how are you?"]
# Check Hindi learning
hindi_prompts = ["à¤¨à¤®à¤¸à¥à¤¤à¥‡", "à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?"]
```

## ğŸŒ Real-World Applications

### Language Accessibility
- ğŸ‡®ğŸ‡³ **India**: 22 official languages
- ğŸ‡®ğŸ‡© **Indonesia**: Bahasa Indonesia
- ğŸ‡§ğŸ‡· **Brazil**: Portuguese variants
- ğŸŒ **Africa**: 2000+ languages

### Domain Adaptation
- ğŸ¥ **Medical**: Clinical terminology
- âš–ï¸ **Legal**: Jurisdiction-specific law
- ğŸ’¼ **Finance**: Industry jargon
- ğŸ”¬ **Science**: Research terminology

### Cultural Context
- ğŸ­ **Idioms**: Culture-specific expressions
- ğŸ“š **Literature**: Classic texts
- ğŸ—£ï¸ **Dialects**: Regional variations
- ğŸ›ï¸ **History**: Cultural knowledge

## ğŸ› Troubleshooting

### Issue: Model outputs gibberish
**Solutions**:
- Check if `modules_to_save` includes `embed_tokens` and `lm_head`
- Increase training steps to 200
- Check data quality (proper Hindi encoding)
- Verify tokenizer handles Hindi

### Issue: Forgetting English
**Solutions**:
- Increase English proportion in data
- Add more bilingual examples
- Reduce learning rate to 1e-4
- Monitor English performance

### Issue: Slow training
**Solutions**:
- Normal! Language learning takes time
- Training embeddings + lm_head is slower
- Use larger GPU (V100/A100)
- Reduce max_seq_length to 384

### Issue: Poor Hindi quality
**Solutions**:
- More training data (need 1000s examples)
- More training steps (try 500+)
- Better quality Hindi corpus
- Native speaker data validation

## ğŸ“š References

### Papers
- [BLOOM](https://arxiv.org/abs/2211.05100) - 46-language model
- [mT5](https://arxiv.org/abs/2010.11934) - Multilingual T5
- [XLM-R](https://arxiv.org/abs/1911.02116) - Cross-lingual model

### Multilingual Datasets
- **OSCAR**: Web crawl (many languages)
- **mC4**: Multilingual C4
- **CC100**: 100+ language corpus
- **Wikipedia**: High-quality multilingual

### Tools & Resources
- [IndicNLP](https://indicnlp.ai4bharat.org/) - Indian language processing
- [AI4Bharat](https://ai4bharat.org/) - Indian language models
- [Samanantar](https://indicnlp.ai4bharat.org/samanantar/) - Hindi-English parallel corpus

## ğŸ“ Learning Outcomes

After completing this notebook, you will understand:

1. **Pre-training vs Fine-tuning**
   - Fundamental knowledge vs task adaptation
   - When to use each approach
   - How they complement each other

2. **Language Learning for AI**
   - Next-token prediction for languages
   - Importance of embeddings and output layers
   - Bilingual training strategies

3. **Practical Implementation**
   - Critical configuration (`modules_to_save`)
   - Preventing catastrophic forgetting
   - Dataset design for languages

4. **Real-World Impact**
   - Making AI accessible globally
   - Supporting underserved languages
   - Cultural preservation through AI

## ğŸŒŸ Key Takeaways

1. **modules_to_save is critical**: Must include `embed_tokens` and `lm_head`
2. **Bilingual prevents forgetting**: Mix languages in training data
3. **More data needed**: 120 examples prove concept, need millions for fluency
4. **Next-token prediction**: Different from instruction fine-tuning
5. **Global accessibility**: This makes AI work for everyone

## ğŸ”— Related Notebooks

- **[Colab 1](../Collab%201/)**: Full fine-tuning - task learning
- **[Colab 2](../Collab%202/)**: LoRA fine-tuning - efficient task learning
- **[Colab 3](../Collab%203/)**: DPO - preference alignment
- **[Colab 4](../Collab%204/)**: GRPO - reasoning models

## ğŸ“– Complete Series Pipeline

```
1. Continued Pre-training (Colab 5) â†’ Learn Hindi
2. SFT (Colab 1-2)                 â†’ Learn tasks in Hindi
3. DPO (Colab 3)                   â†’ Align Hindi preferences
4. GRPO (Colab 4)                  â†’ Add reasoning in Hindi
5. Deploy                          â†’ Bilingual AI system!
```

## ğŸ’¬ Support

Questions about multilingual AI? Want to adapt for other languages? Open an issue!

---

**Status**: âœ… Ready to Run  
**Estimated Time**: 12-15 minutes  
**Difficulty**: Intermediate to Advanced  
**GPU Required**: Yes (T4 or better)  
**Last Updated**: November 9, 2025

**Mission**: Making AI accessible to the 1.3 billion Hindi speakers! ğŸ‡®ğŸ‡³ğŸŒ
