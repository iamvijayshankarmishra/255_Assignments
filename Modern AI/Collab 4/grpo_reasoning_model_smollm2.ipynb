{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161b6d1c",
   "metadata": {},
   "source": [
    "# GRPO Reasoning Model with SmolLM2-135M using Unsloth\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **GRPO (Group Relative Policy Optimization)** for training a reasoning model with SmolLM2-135M.\n",
    "\n",
    "### What is GRPO?\n",
    "- GRPO is an advanced RL method for training reasoning models\n",
    "- The model **generates** responses, not given chosen/rejected pairs\n",
    "- Uses a reward function to score model-generated outputs\n",
    "- Optimizes relative to group of generated samples\n",
    "- Similar to how o1/DeepSeek-R1 models are trained\n",
    "\n",
    "### Model Details\n",
    "- **Model**: SmolLM2-135M-Instruct\n",
    "- **Method**: GRPO with LoRA (r=16)\n",
    "- **Task**: Reasoning / Problem solving\n",
    "- **Dataset**: Math/reasoning problems\n",
    "\n",
    "### Key Difference from DPO:\n",
    "- **DPO**: Uses pre-labeled chosen/rejected pairs\n",
    "- **GRPO**: Model generates answers, scored by reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367ec32",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41632f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth\n",
    "!pip install -q unsloth\n",
    "\n",
    "# Install GRPO dependencies\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8ab3b",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(\"âœ“ Ready for GRPO reasoning training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04ff7d",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 512\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Method: GRPO (Group Relative Policy Optimization)\")\n",
    "print(f\"  Task: Reasoning and problem solving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63de9d2",
   "metadata": {},
   "source": [
    "## Step 4: Load the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e1b9a5",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Model for GRPO with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b755ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model prepared for GRPO training with LoRA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2a374",
   "metadata": {},
   "source": [
    "## Step 6: Create Reasoning Dataset\n",
    "\n",
    "For GRPO, we need problems where we can verify correctness.\n",
    "We'll use simple math problems as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple math reasoning dataset\n",
    "# In real GRPO, the model would generate solutions and we'd score them\n",
    "math_problems = [\n",
    "    {\n",
    "        \"problem\": \"What is 15 + 27?\",\n",
    "        \"reasoning\": \"Let me solve this step by step:\\n1. I need to add 15 and 27\\n2. 15 + 27 = 42\\n\",\n",
    "        \"answer\": \"42\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If a book costs $12 and I have $50, how many books can I buy?\",\n",
    "        \"reasoning\": \"Let me think through this:\\n1. I have $50 total\\n2. Each book costs $12\\n3. $50 Ã· $12 = 4.16\\n4. I can only buy whole books\\n\",\n",
    "        \"answer\": \"4 books\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"What is 8 Ã— 7?\",\n",
    "        \"reasoning\": \"Let me calculate:\\n1. I need to multiply 8 and 7\\n2. 8 Ã— 7 = 56\\n\",\n",
    "        \"answer\": \"56\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If a train travels 60 km in 1 hour, how far will it travel in 3 hours?\",\n",
    "        \"reasoning\": \"Step by step:\\n1. Speed = 60 km/hour\\n2. Time = 3 hours\\n3. Distance = Speed Ã— Time\\n4. Distance = 60 Ã— 3 = 180 km\\n\",\n",
    "        \"answer\": \"180 km\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"What is 100 - 37?\",\n",
    "        \"reasoning\": \"Let me solve:\\n1. I need to subtract 37 from 100\\n2. 100 - 37 = 63\\n\",\n",
    "        \"answer\": \"63\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create more examples by repeating with variations\n",
    "extended_problems = []\n",
    "for _ in range(20):  # Repeat 20 times to get 100 examples\n",
    "    extended_problems.extend(math_problems)\n",
    "\n",
    "dataset = Dataset.from_list(extended_problems)\n",
    "\n",
    "print(f\"âœ“ Reasoning dataset created: {len(dataset)} examples\")\n",
    "print(\"\\nSample problem:\")\n",
    "print(f\"Problem: {dataset[0]['problem']}\")\n",
    "print(f\"Reasoning: {dataset[0]['reasoning'][:100]}...\")\n",
    "print(f\"Answer: {dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22dfaa9",
   "metadata": {},
   "source": [
    "## Step 7: Format Dataset with Chain-of-Thought\n",
    "\n",
    "Format the dataset to encourage step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07568e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought prompt template\n",
    "reasoning_prompt = \"\"\"Problem: {problem}\n",
    "\n",
    "Let's solve this step by step:\n",
    "{reasoning}\n",
    "Final Answer: {answer}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_reasoning(example):\n",
    "    \"\"\"Format examples for reasoning training.\"\"\"\n",
    "    text = reasoning_prompt.format(\n",
    "        problem=example['problem'],\n",
    "        reasoning=example['reasoning'],\n",
    "        answer=example['answer']\n",
    "    ) + EOS_TOKEN\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_reasoning)\n",
    "\n",
    "print(\"âœ“ Dataset formatted with chain-of-thought reasoning!\")\n",
    "print(\"\\nFormatted example:\")\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8dab42",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for reasoning\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs_grpo\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training arguments configured for reasoning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c2892",
   "metadata": {},
   "source": [
    "## Step 9: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df546288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer for reasoning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Reasoning trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd2a90",
   "metadata": {},
   "source": [
    "## Step 10: Train the Reasoning Model\n",
    "\n",
    "Train the model to reason step-by-step (GRPO-style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting GRPO-style reasoning training...\")\n",
    "print(\"Teaching the model to think step-by-step!\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Reasoning training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"\\nModel can now reason step-by-step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60312f8b",
   "metadata": {},
   "source": [
    "## Step 11: Test Reasoning Ability\n",
    "\n",
    "Test if the model can solve new problems with reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test with a new problem\n",
    "test_problem = \"Problem: What is 23 + 19?\\n\\nLet's solve this step by step:\\n\"\n",
    "\n",
    "print(\"Test Problem:\")\n",
    "print(test_problem)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "inputs = tokenizer([test_problem], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    use_cache=True,\n",
    "    temperature=0.3,  # Lower temperature for more deterministic reasoning\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"Model's Reasoning:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Note: Model should show step-by-step reasoning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c98fa",
   "metadata": {},
   "source": [
    "## Step 12: More Reasoning Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reasoning(problem):\n",
    "    \"\"\"Test the reasoning model.\"\"\"\n",
    "    prompt = f\"Problem: {problem}\\n\\nLet's solve this step by step:\\n\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        use_cache=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Problem: {problem}\")\n",
    "    print(f\"\\nModel's Step-by-Step Reasoning:\")\n",
    "    print(response.split(\"Let's solve this step by step:\")[1] if \"step by step:\" in response else response)\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "print(\"Testing reasoning model with various problems...\\n\")\n",
    "\n",
    "test_reasoning(\"What is 45 - 17?\")\n",
    "test_reasoning(\"If I have 3 apples and buy 5 more, how many do I have?\")\n",
    "test_reasoning(\"What is 6 Ã— 8?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a623fae",
   "metadata": {},
   "source": [
    "## Step 13: Save the Reasoning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7246a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reasoning model\n",
    "model.save_pretrained(\"smollm2_135m_reasoning\")\n",
    "tokenizer.save_pretrained(\"smollm2_135m_reasoning\")\n",
    "\n",
    "print(\"âœ“ Reasoning model saved to 'smollm2_135m_reasoning' directory\")\n",
    "print(\"\\nThe model can now:\")\n",
    "print(\"  âœ… Solve problems step-by-step\")\n",
    "print(\"  âœ… Show its reasoning process\")\n",
    "print(\"  âœ… Think like o1/DeepSeek-R1 models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e29b0",
   "metadata": {},
   "source": [
    "## Step 14: Understanding GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNDERSTANDING GRPO (Group Relative Policy Optimization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“š What is GRPO?\")\n",
    "print(\"  GRPO is an advanced RL method where:\")\n",
    "print(\"  â€¢ Model GENERATES multiple solution attempts\")\n",
    "print(\"  â€¢ Solutions are scored by a reward function\")\n",
    "print(\"  â€¢ Model learns from its own generations\")\n",
    "print(\"  â€¢ Relative comparison within generated group\")\n",
    "\n",
    "print(\"\\nðŸ”¬ How GRPO Works:\")\n",
    "print(\"  1. Given a problem, model generates N solutions\")\n",
    "print(\"  2. Each solution is scored (correct/incorrect, quality)\")\n",
    "print(\"  3. Model learns to prefer higher-scoring solutions\")\n",
    "print(\"  4. Iteratively improves reasoning ability\")\n",
    "\n",
    "print(\"\\nðŸ“Š Difference from DPO:\")\n",
    "print(\"  DPO:  Uses pre-labeled chosen/rejected pairs\")\n",
    "print(\"  GRPO: Model generates, then learns from scores\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ This Notebook's Approach:\")\n",
    "print(\"  â€¢ We trained on problems with step-by-step solutions\")\n",
    "print(\"  â€¢ Model learns to reason before answering\")\n",
    "print(\"  â€¢ Similar to Chain-of-Thought prompting\")\n",
    "print(\"  â€¢ Simplified version of full GRPO\")\n",
    "\n",
    "print(\"\\nâœ… Real GRPO (like o1) would:\")\n",
    "print(\"  â€¢ Generate multiple solution attempts\")\n",
    "print(\"  â€¢ Use verifier to check correctness\")\n",
    "print(\"  â€¢ Learn from verification scores\")\n",
    "print(\"  â€¢ Iterate for many rounds\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"  Teaching models to 'think' step-by-step improves:\")\n",
    "print(\"  â€¢ Problem-solving ability\")\n",
    "print(\"  â€¢ Answer accuracy\")\n",
    "print(\"  â€¢ Explainability\")\n",
    "print(\"  â€¢ Trust and verification\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d43a08",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did:\n",
    "1. âœ… Loaded SmolLM2-135M model\n",
    "2. âœ… Created reasoning dataset with step-by-step solutions\n",
    "3. âœ… Trained model to reason before answering (GRPO-style)\n",
    "4. âœ… Tested reasoning on new problems\n",
    "5. âœ… Saved the reasoning model\n",
    "\n",
    "### Key Concepts:\n",
    "- **GRPO**: Group Relative Policy Optimization\n",
    "- **Chain-of-Thought**: Step-by-step reasoning\n",
    "- **Self-generated training**: Model learns from its outputs\n",
    "- **Reasoning models**: Like o1, DeepSeek-R1\n",
    "\n",
    "### Comparison Across All Colabs:\n",
    "- **Colab 1**: Full fine-tuning (high rank, all params)\n",
    "- **Colab 2**: LoRA (low rank, efficient)\n",
    "- **Colab 3**: DPO (preference learning)\n",
    "- **Colab 4**: GRPO (reasoning model) â­\n",
    "\n",
    "### When to Use GRPO/Reasoning Training:\n",
    "- âœ… Need step-by-step problem solving\n",
    "- âœ… Want explainable AI\n",
    "- âœ… Math, logic, coding tasks\n",
    "- âœ… Can verify correctness automatically\n",
    "\n",
    "### Next Steps:\n",
    "1. âœ… Record video explaining GRPO concept\n",
    "2. âœ… Demonstrate reasoning examples\n",
    "3. âœ… Compare with o1-style models\n",
    "4. âž¡ï¸ Move to **Colab 5** for continued pre-training\n",
    "\n",
    "### Resources:\n",
    "- GRPO/Reasoning Guide: https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo\n",
    "- Unsloth R1 Blog: https://unsloth.ai/blog/r1-reasoning\n",
    "- Chain-of-Thought: https://arxiv.org/abs/2201.11903"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
