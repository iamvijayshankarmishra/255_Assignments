{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58703ef8",
   "metadata": {},
   "source": [
    "# DPO Reinforcement Learning with SmolLM2-135M using Unsloth\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **DPO (Direct Preference Optimization)** for reinforcement learning with SmolLM2-135M using Unsloth.ai.\n",
    "\n",
    "### What is DPO?\n",
    "- DPO is a reinforcement learning method for aligning LLMs with human preferences\n",
    "- Uses pairs of **preferred** (good) and **rejected** (bad) outputs\n",
    "- Simpler than PPO (no reward model needed)\n",
    "- Directly optimizes the policy to prefer better responses\n",
    "\n",
    "### Model Details\n",
    "- **Model**: SmolLM2-135M-Instruct\n",
    "- **Method**: DPO with LoRA (r=16)\n",
    "- **Task**: Preference learning / Response alignment\n",
    "- **Dataset**: Anthropic HH-RLHF with preferred/rejected pairs\n",
    "\n",
    "### Key Concepts\n",
    "- **Chosen**: The better/desired response\n",
    "- **Rejected**: The worse/undesired response\n",
    "- **DPO Loss**: Maximizes probability of chosen, minimizes rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa24634",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Install Unsloth and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c79861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth\n",
    "!pip install -q unsloth\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q --no-deps \"trl>=0.9.6\" xformers peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c44ce7",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Disable Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(\"âœ“ Wandb disabled for uninterrupted training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6eab3f",
   "metadata": {},
   "source": [
    "## Step 3: Create Custom Educational Preference Dataset\n",
    "\n",
    "We'll create a custom dataset with educational Q&A pairs showing helpful vs unhelpful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom educational preference dataset\n",
    "print(\"Creating custom educational preference dataset...\")\n",
    "\n",
    "preference_data = [\n",
    "    # Programming & Technology\n",
    "    {\"prompt\": \"What is Python?\", \n",
    "     \"chosen\": \"Python is a high-level, interpreted programming language known for its simple syntax and readability. It's widely used in web development, data science, AI, automation, and scientific computing. Python emphasizes code readability with significant indentation.\",\n",
    "     \"rejected\": \"It's a programming language.\"},\n",
    "    \n",
    "    {\"prompt\": \"How do I start learning to code?\",\n",
    "     \"chosen\": \"Start by choosing a beginner-friendly language like Python. Learn basic concepts (variables, loops, functions), practice daily with small projects, use interactive platforms like Codecademy or freeCodeCamp, and join coding communities for support. Build real projects to reinforce learning.\",\n",
    "     \"rejected\": \"Just pick a language and start coding.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is machine learning?\",\n",
    "     \"chosen\": \"Machine learning is a subset of AI where computers learn from data without explicit programming. It uses algorithms to identify patterns, make decisions, and improve performance over time. Common types include supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "     \"rejected\": \"It's when computers learn stuff automatically.\"},\n",
    "    \n",
    "    {\"prompt\": \"Explain what an algorithm is.\",\n",
    "     \"chosen\": \"An algorithm is a step-by-step procedure or set of rules designed to solve a specific problem or perform a task. Like a recipe, it has defined inputs, processes, and expected outputs. Algorithms are fundamental to computer science and programming.\",\n",
    "     \"rejected\": \"It's computer instructions.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is the difference between AI and ML?\",\n",
    "     \"chosen\": \"AI (Artificial Intelligence) is the broader concept of machines performing tasks that typically require human intelligence. ML (Machine Learning) is a subset of AI focused on systems that learn from data. All ML is AI, but not all AI uses ML - some AI uses rule-based systems.\",\n",
    "     \"rejected\": \"They're basically the same thing.\"},\n",
    "    \n",
    "    # Mathematics & Science\n",
    "    {\"prompt\": \"What is calculus used for?\",\n",
    "     \"chosen\": \"Calculus is used to study change and motion. It has two main branches: differential calculus (rates of change, slopes) and integral calculus (accumulation, areas). It's essential in physics, engineering, economics, statistics, and computer graphics.\",\n",
    "     \"rejected\": \"It's for math problems.\"},\n",
    "    \n",
    "    {\"prompt\": \"Explain the scientific method.\",\n",
    "     \"chosen\": \"The scientific method is a systematic approach to research: 1) Observe and question, 2) Research background, 3) Form a hypothesis, 4) Design and conduct experiments, 5) Analyze data, 6) Draw conclusions, 7) Communicate results. It ensures objective, reproducible findings.\",\n",
    "     \"rejected\": \"You make a guess and test it.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is probability?\",\n",
    "     \"chosen\": \"Probability is the mathematical study of likelihood and uncertainty. It quantifies how likely events are to occur, expressed as numbers between 0 (impossible) and 1 (certain). It's used in statistics, risk assessment, decision-making, and predictions.\",\n",
    "     \"rejected\": \"It's about chances of things happening.\"},\n",
    "    \n",
    "    # Study & Learning\n",
    "    {\"prompt\": \"How can I improve my study habits?\",\n",
    "     \"chosen\": \"Effective study strategies include: 1) Set specific goals and schedules, 2) Use active recall and spaced repetition, 3) Take regular breaks (Pomodoro technique), 4) Teach concepts to others, 5) Minimize distractions, 6) Sleep well and stay hydrated. Quality matters more than quantity.\",\n",
    "     \"rejected\": \"Just study more hours.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is critical thinking?\",\n",
    "     \"chosen\": \"Critical thinking is the objective analysis and evaluation of information to form reasoned judgments. It involves questioning assumptions, identifying biases, analyzing evidence, considering alternatives, and drawing logical conclusions. It's essential for problem-solving and decision-making.\",\n",
    "     \"rejected\": \"It means thinking hard about things.\"},\n",
    "    \n",
    "    {\"prompt\": \"How do I manage academic stress?\",\n",
    "     \"chosen\": \"Manage academic stress by: 1) Organizing tasks with a planner, 2) Breaking large projects into smaller steps, 3) Practicing time management, 4) Taking regular breaks and exercise, 5) Seeking support from peers or counselors, 6) Maintaining work-life balance, 7) Using relaxation techniques. Remember, asking for help is strength.\",\n",
    "     \"rejected\": \"Just work harder and sleep less.\"},\n",
    "    \n",
    "    # Career & Skills\n",
    "    {\"prompt\": \"What skills are important for software engineers?\",\n",
    "     \"chosen\": \"Essential software engineering skills include: 1) Programming proficiency (multiple languages), 2) Data structures and algorithms, 3) Problem-solving and debugging, 4) Version control (Git), 5) Testing and documentation, 6) Communication and teamwork, 7) Continuous learning mindset.\",\n",
    "     \"rejected\": \"You just need to know coding.\"},\n",
    "    \n",
    "    {\"prompt\": \"How do I prepare for technical interviews?\",\n",
    "     \"chosen\": \"Prepare for technical interviews by: 1) Practicing coding problems on LeetCode/HackerRank, 2) Understanding data structures and algorithms deeply, 3) Doing mock interviews with peers, 4) Reviewing system design concepts, 5) Preparing behavioral questions, 6) Understanding the company and role, 7) Asking thoughtful questions.\",\n",
    "     \"rejected\": \"Just memorize coding problems.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is teamwork?\",\n",
    "     \"chosen\": \"Teamwork is collaborative effort toward a common goal. Effective teamwork requires clear communication, mutual respect, defined roles, shared accountability, trust, and constructive conflict resolution. Good team members listen actively, contribute ideas, support others, and adapt to different working styles.\",\n",
    "     \"rejected\": \"Working with other people on projects.\"},\n",
    "    \n",
    "    # Additional educational topics\n",
    "    {\"prompt\": \"What is data science?\",\n",
    "     \"chosen\": \"Data science combines statistics, programming, and domain expertise to extract insights from data. It involves data collection, cleaning, analysis, visualization, and modeling. Data scientists use tools like Python, R, SQL, and machine learning to solve real-world problems and inform decisions.\",\n",
    "     \"rejected\": \"It's about working with data.\"},\n",
    "    \n",
    "    {\"prompt\": \"Explain cloud computing.\",\n",
    "     \"chosen\": \"Cloud computing delivers computing services (servers, storage, databases, software) over the internet. Instead of owning physical infrastructure, users rent resources on-demand. Major types include IaaS, PaaS, and SaaS. Benefits include scalability, cost-efficiency, and accessibility from anywhere.\",\n",
    "     \"rejected\": \"Storing stuff on the internet.\"},\n",
    "    \n",
    "    {\"prompt\": \"What are databases used for?\",\n",
    "     \"chosen\": \"Databases store, organize, and manage structured data efficiently. They enable quick data retrieval, updates, and queries. Types include relational (SQL) and non-relational (NoSQL) databases. They're essential for applications, websites, businesses, and any system handling significant data.\",\n",
    "     \"rejected\": \"They store information.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is cybersecurity?\",\n",
    "     \"chosen\": \"Cybersecurity protects computer systems, networks, and data from digital attacks, unauthorized access, and damage. It includes practices like encryption, firewalls, authentication, security audits, and incident response. As technology grows, cybersecurity becomes increasingly critical for individuals and organizations.\",\n",
    "     \"rejected\": \"Keeping computers safe from hackers.\"},\n",
    "    \n",
    "    {\"prompt\": \"How does the internet work?\",\n",
    "     \"chosen\": \"The internet is a global network of interconnected computers communicating via standardized protocols (TCP/IP). Data is broken into packets, routed through multiple servers and routers, then reassembled at the destination. Key components include ISPs, DNS, servers, and various network infrastructure.\",\n",
    "     \"rejected\": \"Computers connected worldwide.\"},\n",
    "    \n",
    "    {\"prompt\": \"What is version control?\",\n",
    "     \"chosen\": \"Version control systems (like Git) track changes to files over time, enabling collaboration, history tracking, and rollback capabilities. They allow multiple developers to work simultaneously, manage different versions, and merge changes. Essential for software development and any collaborative document work.\",\n",
    "     \"rejected\": \"Tracking file changes.\"},\n",
    "    \n",
    "    {\"prompt\": \"Explain object-oriented programming.\",\n",
    "     \"chosen\": \"Object-Oriented Programming (OOP) is a paradigm organizing code into objects containing data (attributes) and behaviors (methods). Core principles include encapsulation, inheritance, polymorphism, and abstraction. OOP promotes code reusability, modularity, and easier maintenance.\",\n",
    "     \"rejected\": \"Programming with objects.\"},\n",
    "]\n",
    "\n",
    "# Convert to dataset format\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(preference_data)\n",
    "\n",
    "print(f\"âœ“ Custom dataset created: {len(dataset)} preference pairs\")\n",
    "print(\"\\nDataset format:\")\n",
    "print(\"  - 'prompt': Question or instruction\")\n",
    "print(\"  - 'chosen': Detailed, helpful, educational response\")\n",
    "print(\"  - 'rejected': Brief, unhelpful, vague response\")\n",
    "print(\"\\nTopics covered: Programming, AI/ML, Math, Study Skills, Career\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d3a76",
   "metadata": {},
   "source": [
    "## Step 4: Examine Sample Preference Pairs\n",
    "\n",
    "Let's look at examples of chosen vs rejected responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example Preference Pair #1:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PROMPT: {dataset[0]['prompt']}\")\n",
    "print(f\"\\nCHOSEN (Detailed, Helpful):\")\n",
    "print(dataset[0]['chosen'])\n",
    "print(f\"\\nREJECTED (Brief, Unhelpful):\")\n",
    "print(dataset[0]['rejected'])\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\\nExample Preference Pair #2:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PROMPT: {dataset[1]['prompt']}\")\n",
    "print(f\"\\nCHOSEN (Detailed, Helpful):\")\n",
    "print(dataset[1]['chosen'])\n",
    "print(f\"\\nREJECTED (Brief, Unhelpful):\")\n",
    "print(dataset[1]['rejected'])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a09cbf",
   "metadata": {},
   "source": [
    "## Step 5: Verify Dataset Format\n",
    "\n",
    "The dataset is already in the correct format for DPO training (prompt, chosen, rejected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset format\n",
    "print(\"âœ“ Dataset is ready for DPO training\")\n",
    "print(f\"  Total examples: {len(dataset)}\")\n",
    "print(f\"  Fields: {dataset.column_names}\")\n",
    "print(\"\\nSample structure:\")\n",
    "print(f\"  Prompt length (avg): {sum(len(x['prompt']) for x in dataset) // len(dataset)} characters\")\n",
    "print(f\"  Chosen length (avg): {sum(len(x['chosen']) for x in dataset) // len(dataset)} characters\")\n",
    "print(f\"  Rejected length (avg): {sum(len(x['rejected']) for x in dataset) // len(dataset)} characters\")\n",
    "print(\"\\nâœ“ All examples have prompt, chosen, and rejected fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7a25d",
   "metadata": {},
   "source": [
    "## Step 6: Load Model and Tokenizer\n",
    "\n",
    "Load SmolLM2-135M with 4-bit quantization for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "max_seq_length = 512  # Shorter for DPO\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model and tokenizer loaded successfully!\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Max sequence length: {max_seq_length}\")\n",
    "print(f\"  Quantization: 4-bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d51cc6",
   "metadata": {},
   "source": [
    "## Step 7: Configure LoRA for DPO\n",
    "\n",
    "Apply LoRA adapters for parameter-efficient DPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2175d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"âœ“ LoRA applied successfully!\")\n",
    "print(f\"  LoRA rank: 16\")\n",
    "print(f\"  LoRA alpha: 16\")\n",
    "print(f\"  Target modules: Attention + MLP layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267f842",
   "metadata": {},
   "source": [
    "## Step 8: Configure DPO Training Arguments\n",
    "\n",
    "Set up training parameters for DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3650c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DPO training\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=60,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    # DPO-specific parameters\n",
    "    beta=0.1,  # DPO temperature\n",
    "    max_prompt_length=256,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "print(\"âœ“ DPO training arguments configured!\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Beta (DPO temperature): {training_args.beta}\")\n",
    "print(f\"  Batch size: 2 Ã— 4 = 8 (effective)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65912584",
   "metadata": {},
   "source": [
    "## Step 9: Initialize DPO Trainer\n",
    "\n",
    "Create the DPO trainer with our model and preference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # Will use the base model as reference\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=256,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "print(\"âœ“ DPO Trainer initialized successfully!\")\n",
    "print(f\"  Training pairs: {len(dataset)}\")\n",
    "print(f\"  Reference model: Using base model internally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e038955",
   "metadata": {},
   "source": [
    "## Step 10: Train with DPO\n",
    "\n",
    "Start DPO training to align the model with human preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting DPO training...\")\n",
    "print(\"The model will learn to prefer 'chosen' over 'rejected' responses!\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = dpo_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ DPO Training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
    "print(f\"\\nThe model is now aligned with human preferences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960bf51",
   "metadata": {},
   "source": [
    "## Step 11: Test the DPO-Trained Model\n",
    "\n",
    "Let's test if the model generates more helpful and harmless responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"Should I trust everything I read online?\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Test: Safety and Critical Thinking\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca046ff3",
   "metadata": {},
   "source": [
    "## Step 12: Test with Another Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_2 = \"\"\"I'm feeling stressed about work.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_prompt_2,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Test: Emotional Support and Helpfulness\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: {test_prompt_2}\")\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d35ca2",
   "metadata": {},
   "source": [
    "## Step 13: Test with Constructive Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fed376",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_3 = \"\"\"How can I win an argument?\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_prompt_3,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Test: Constructive vs Aggressive Advice\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: {test_prompt_3}\")\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: After DPO, the model should suggest respectful\")\n",
    "print(\"communication rather than aggressive tactics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eef23",
   "metadata": {},
   "source": [
    "## Step 14: Save the DPO Model\n",
    "\n",
    "Save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"smollm2_dpo\")\n",
    "tokenizer.save_pretrained(\"smollm2_dpo\")\n",
    "\n",
    "print(\"âœ“ Model saved successfully!\")\n",
    "print(\"  Location: ./smollm2_dpo\")\n",
    "print(\"  Format: LoRA adapters + tokenizer\")\n",
    "print(\"\\nYou can load this model later with:\")\n",
    "print(\"  model, tokenizer = FastLanguageModel.from_pretrained('smollm2_dpo')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e36a1",
   "metadata": {},
   "source": [
    "## Step 15: Optional - Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4354522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save merged model (base + adapters)\n",
    "model.save_pretrained_merged(\n",
    "    \"smollm2_dpo_merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Merged model saved!\")\n",
    "print(\"  Location: ./smollm2_dpo_merged\")\n",
    "print(\"  Format: Complete model (ready for inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad2b4f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did:\n",
    "1. âœ… Loaded Anthropic HH-RLHF preference dataset\n",
    "2. âœ… Prepared chosen vs rejected response pairs\n",
    "3. âœ… Applied LoRA for efficient training\n",
    "4. âœ… Trained with DPO to prefer helpful responses\n",
    "5. âœ… Tested alignment with safety/helpfulness prompts\n",
    "6. âœ… Saved DPO-aligned model\n",
    "\n",
    "### Key Takeaways:\n",
    "- **DPO** directly optimizes preferences without a reward model\n",
    "- **Beta parameter** (0.1) controls preference strength\n",
    "- **Helpful & Harmless** are key alignment goals\n",
    "- Model now prefers better responses over worse ones\n",
    "\n",
    "### Next Steps:\n",
    "- Try with larger datasets (10K+ pairs)\n",
    "- Experiment with beta values (0.05-0.5)\n",
    "- Test on your own preference pairs\n",
    "- Deploy for real-world applications\n",
    "\n",
    "**Congratulations!** You've successfully trained a preference-aligned model with DPO! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
