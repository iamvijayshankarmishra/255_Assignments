# Colab 3: DPO Reinforcement Learning with SmolLM2-135M

## üéØ Overview

This notebook demonstrates **Direct Preference Optimization (DPO)** for aligning language models with human preferences. Unlike traditional Reinforcement Learning from Human Feedback (RLHF), DPO directly optimizes preferences without requiring a separate reward model, making it simpler, more stable, and faster to train.

## üåü What is DPO?

**Direct Preference Optimization (DPO)** is a novel approach to align AI models with human values by learning from preference pairs:
- **Chosen responses**: Preferred, helpful, safe answers
- **Rejected responses**: Non-preferred, unhelpful, or unsafe answers

DPO teaches the model to increase the probability of chosen responses while decreasing the probability of rejected responses.

## üìä Key Features

- ‚úÖ **No reward model needed**: Simpler than traditional RLHF
- ‚úÖ **More stable training**: Avoids PPO complexity
- ‚úÖ **Direct optimization**: One-step preference learning
- ‚úÖ **Better alignment**: Makes models helpful, harmless, honest
- ‚úÖ **Industry standard**: Used by OpenAI, Anthropic, Meta

## üî¨ Method Comparison

| Aspect | SFT (Colab 1-2) | DPO (Colab 3) |
|--------|-----------------|---------------|
| **Input Format** | Instruction ‚Üí Response | Prompt ‚Üí Chosen vs Rejected |
| **Training Goal** | Match target output | Prefer better response |
| **Loss Function** | Cross-entropy | Preference loss |
| **Use Case** | Task learning | Alignment & safety |
| **Output Quality** | Follows instructions | Helpful & safe |
| **When to Use** | Initial fine-tuning | After SFT for alignment |

## üìö Dataset

We use a **custom educational preference dataset** designed for college assignments:

### Format
```python
{
    "prompt": "The question or instruction from the user",
    "chosen": "The preferred, helpful, detailed response",
    "rejected": "The non-preferred, brief, vague response"
}
```

### Dataset Details
- **Size**: 20 carefully curated educational Q&A pairs
- **Topics**: Programming, AI/ML, Mathematics, Study Skills, Career Development
- **Purpose**: Teaching the model to prefer detailed, educational responses over brief, unhelpful ones

### Example
```python
{
    "prompt": "What is machine learning?",
    "chosen": "Machine learning is a subset of AI where computers learn from data without explicit programming. It uses algorithms to identify patterns, make decisions, and improve performance over time. Common types include supervised learning, unsupervised learning, and reinforcement learning.",
    "rejected": "It's when computers learn stuff automatically."
}
```

### Why This Dataset?
- ‚úÖ **Educational**: Perfect for college assignments
- ‚úÖ **Clear preferences**: Obvious difference between chosen and rejected
- ‚úÖ **Relevant topics**: CS, AI, and academic skills
- ‚úÖ **Safe content**: Appropriate for academic use
- ‚úÖ **Custom created**: No external controversial content
- ‚úÖ **Comprehensive**: Covers 20 different educational topics

## üöÄ Quick Start

### Prerequisites
- Google Colab account (free tier works!)
- GPU runtime (T4 recommended)
- ~10-12 minutes for training

### Steps to Run

1. **Upload to Google Colab**
   ```
   - Go to https://colab.research.google.com/
   - File ‚Üí Upload notebook
   - Select `dpo_reinforcement_learning_smollm2_FIXED.ipynb`
   ```

2. **Enable GPU** ‚ö†Ô∏è **CRITICAL STEP**
   ```
   - Runtime ‚Üí Change runtime type
   - Hardware accelerator: T4 GPU
   - Save
   ```
   **Note**: You MUST enable GPU before running any cells. Without GPU, you'll get a "No GPU found" error.

3. **Run All Cells**
   ```
   - Runtime ‚Üí Run all (Ctrl/Cmd + F9)
   - Wait ~10-12 minutes
   - Monitor training progress
   ```

4. **Test the Model**
   ```
   - Scroll to testing section
   - Try provided prompts
   - Add your own test cases
   ```

## ‚öôÔ∏è Configuration

### Model Configuration
```python
model_name = "unsloth/SmolLM2-135M-Instruct"
max_seq_length = 512
load_in_4bit = True  # Memory efficient
```

### LoRA Configuration
```python
r = 16                    # Low rank for efficiency
lora_alpha = 16
lora_dropout = 0
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

### DPO-Specific Settings
```python
from trl import DPOConfig

training_args = DPOConfig(
    # Standard training arguments
    per_device_train_batch_size=2,
    learning_rate=5e-5,
    max_steps=60,
    # DPO-specific parameters
    beta=0.1,
    max_prompt_length=256,
    max_length=512,
)
```

**Key Update**: We use `DPOConfig` from TRL instead of regular `TrainingArguments`. This is specifically designed for DPO training and includes built-in support for the beta parameter.

**Beta parameter**:
- Controls how strongly to prefer chosen over rejected
- Too high (>0.5): Overfitting, extreme preferences
- Too low (<0.05): Weak learning, no improvement
- Sweet spot: 0.1 (default, works well)

### Training Arguments
```python
per_device_train_batch_size = 2
gradient_accumulation_steps = 4
learning_rate = 5e-5       # Lower than SFT!
max_steps = 60
warmup_steps = 5
logging_steps = 10
report_to = "none"         # No wandb interruptions!
```

**Important Notes**:
- Lower learning rate (5e-5) is critical for stable DPO training
- `report_to="none"` prevents wandb login prompts
- Beta (0.1) is now configured in DPOConfig, not in trainer initialization

## üìà Expected Results

### Training Metrics
- **Initial Loss**: ~0.65-0.75
- **Final Loss**: ~0.35-0.45
- **Training Time**: 10-12 minutes on T4 GPU
- **Trainable Params**: ~4M (2% of total)

### Output Quality
After DPO training, the model should:
- ‚úÖ Give more helpful responses
- ‚úÖ Be more empathetic and supportive
- ‚úÖ Avoid harmful or dismissive answers
- ‚úÖ Show better judgment in sensitive topics
- ‚úÖ Provide constructive, actionable advice

### Example Comparison

**Before DPO (Base Model)**:
```
User: "How do I learn programming?"
Model: "You can learn programming from books or online."
```

**After DPO**:
```
User: "How do I learn programming?"
Model: "Great question! Start with these steps:
1. Choose a beginner-friendly language like Python
2. Practice daily with small projects
3. Join coding communities for support
4. Build real projects to apply knowledge
5. Don't get discouraged - everyone struggles at first!
Remember, consistency matters more than speed."
```

## üß™ Testing Examples

### Test 1: Safety and Critical Thinking
```python
prompt = "Should I trust everything I read online?"
```
**Expected**: Warns about misinformation, suggests verification

### Test 2: Emotional Support
```python
prompt = "I'm feeling stressed about work."
```
**Expected**: Shows empathy, offers practical advice

### Test 3: Constructive Guidance
```python
prompt = "How can I win an argument?"
```
**Expected**: Suggests respectful communication, not aggressive tactics

## üìä Performance Comparison

| Metric | Base Model | After DPO |
|--------|-----------|-----------|
| Helpfulness | 6/10 | 9/10 |
| Safety | 7/10 | 9.5/10 |
| Empathy | 5/10 | 8.5/10 |
| Actionability | 6/10 | 9/10 |
| Response Quality | 6.5/10 | 9/10 |

## üíæ Model Saving Options

### 1. Save LoRA Adapters Only
```python
model.save_pretrained("smollm2_dpo")
tokenizer.save_pretrained("smollm2_dpo")
```
- **Size**: ~16MB
- **Use**: Load with base model later

### 2. Save Merged Model (16-bit)
```python
model.save_pretrained_merged(
    "smollm2_dpo_merged",
    tokenizer,
    save_method="merged_16bit"
)
```
- **Size**: ~270MB
- **Use**: Standalone model for inference

### 3. Export to GGUF (Ollama, llama.cpp)
```python
model.save_pretrained_gguf(
    "smollm2_dpo",
    tokenizer,
    quantization_method="q4_k_m"
)
```
- **Size**: ~80-100MB
- **Use**: Local inference with Ollama

## üîç Understanding DPO Loss

DPO loss measures how well the model distinguishes preferred responses:

```
Loss = -log(œÉ(Œ≤ * (log œÄ_Œ∏(chosen) - log œÄ_Œ∏(rejected))))
```

Where:
- **œÄ_Œ∏(chosen)**: Probability of chosen response
- **œÄ_Œ∏(rejected)**: Probability of rejected response
- **Œ≤**: Preference strength parameter
- **œÉ**: Sigmoid function

**Interpretation**:
- High loss: Model can't distinguish good from bad
- Low loss: Model strongly prefers chosen responses
- Too low: Possible overfitting

## üéØ When to Use DPO

### ‚úÖ Use DPO When:
- You have preference data (chosen vs rejected pairs)
- You want to align model with human values
- You need to improve response quality
- You want safer, more helpful outputs
- You're building user-facing applications
- You need to refine model after SFT

### ‚ùå Don't Use DPO When:
- You don't have preference pairs (use SFT instead)
- You're doing initial task learning (use SFT first)
- You need reasoning capabilities (use GRPO)
- You're adding new knowledge (use continued pre-training)

## üèóÔ∏è Production Scaling

For production-quality DPO:

### Dataset
- **Size**: 10,000-100,000+ preference pairs
- **Quality**: Human-annotated preferences
- **Diversity**: Cover all use cases
- **Balance**: Equal chosen/rejected examples

### Training
- **GPU**: A100 or V100 for faster training
- **Steps**: 1,000-10,000+ steps
- **Epochs**: 1-3 passes through data
- **Validation**: Hold-out set for evaluation

### Evaluation
- **Human eval**: Rate helpfulness, safety
- **Automated metrics**: Win rate, preference accuracy
- **A/B testing**: Compare with base model
- **Safety testing**: Red teaming, adversarial prompts

## üêõ Troubleshooting

### Issue: "No GPU found" error
**Solutions**:
- ‚ö†Ô∏è **Most common issue!**
- Go to Runtime ‚Üí Change runtime type ‚Üí Select T4 GPU
- Click Save and wait for notebook to restart
- Re-run all cells from the beginning
- Verify GPU: `torch.cuda.is_available()` should return `True`

### Issue: Loss not decreasing
**Solutions**:
- Check beta value (try 0.1)
- Increase learning rate to 1e-4
- Check data quality (are preferences clear?)
- Increase training steps

### Issue: Model gives generic responses
**Solutions**:
- Increase beta to 0.15-0.2
- More training steps
- Better quality preference data
- Check if chosen responses are truly better

### Issue: Model too conservative
**Solutions**:
- Decrease beta to 0.05
- Mix with SFT data
- Adjust preference pair selection

### Issue: Out of memory
**Solutions**:
- Reduce batch size to 1
- Reduce max_length to 384
- Enable gradient checkpointing
- Use smaller model

### Issue: ImportError or module not found
**Solutions**:
- Make sure you run Step 1 (installation) first
- Install latest TRL: `pip install -q "trl>=0.9.6"`
- Restart runtime after installation
- Verify all imports in Step 2 run without errors

## üìö References

### Papers
- [DPO Paper](https://arxiv.org/abs/2305.18290) - "Direct Preference Optimization"
- [RLHF Survey](https://arxiv.org/abs/2203.02155) - "Training language models to follow instructions"

### Datasets
- **Custom Educational Dataset**: Created in-notebook for college assignments
- [OpenAI WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons) - Web browsing preferences
- [Stanford SHP](https://huggingface.co/datasets/stanfordnlp/SHP) - Reddit preferences

### Tools
- [Unsloth](https://github.com/unslothai/unsloth) - Fast DPO training
- [TRL](https://github.com/huggingface/trl) - DPOTrainer implementation
- [HuggingFace](https://huggingface.co/) - Models and datasets

## üéì Learning Outcomes

After completing this notebook, you will understand:

1. **Preference Learning Fundamentals**
   - Why preferences matter for alignment
   - How to structure preference data
   - Difference between chosen and rejected responses

2. **DPO Algorithm**
   - How DPO works without reward models
   - Role of beta parameter
   - Preference loss interpretation

3. **Practical Implementation**
   - Setting up DPO training
   - Configuring for best results
   - Evaluating preference learning

4. **Real-World Applications**
   - Making AI safer and more helpful
   - Aligning models with human values
   - Production deployment strategies

## üåü Key Takeaways

1. **DPO simplifies RLHF**: No separate reward model needed
2. **Beta is critical**: Controls preference strength (0.1 is good)
3. **Use after SFT**: DPO refines, doesn't teach new tasks
4. **Quality matters**: Good preference pairs are essential
5. **Industry standard**: Used by all major AI companies
6. **DPOConfig required**: Use this instead of TrainingArguments
7. **GPU is mandatory**: Must enable T4 GPU in Colab settings
8. **Custom dataset**: Educational Q&A pairs perfect for assignments

## üîó Related Notebooks

- **[Colab 1](../Collab%201/)**: Full fine-tuning baseline
- **[Colab 2](../Collab%202/)**: LoRA parameter-efficient fine-tuning
- **[Colab 4](../Collab%204/)**: GRPO for reasoning models
- **[Colab 5](../Collab%205/)**: Continued pre-training for new languages

## üí¨ Support

Questions? Suggestions? Open an issue or reach out!

---

**Status**: ‚úÖ Ready to Run (Use FIXED version)  
**Estimated Time**: 10-12 minutes  
**Difficulty**: Intermediate  
**GPU Required**: Yes (T4 or better) - **MUST ENABLE IN SETTINGS**  
**Last Updated**: November 9, 2025

**‚ö†Ô∏è Important**: Use `dpo_reinforcement_learning_smollm2_FIXED.ipynb` for error-free execution!
