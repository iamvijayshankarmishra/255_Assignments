{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564cf104",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning with SmolLM2-135M using Unsloth\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **LoRA (Low-Rank Adaptation)** fine-tuning of the SmolLM2-135M model using Unsloth.ai.\n",
    "\n",
    "### What is LoRA?\n",
    "- LoRA is a **parameter-efficient** fine-tuning method\n",
    "- Updates only a small number of adapter parameters (< 1% of model)\n",
    "- Much faster and memory efficient than full fine-tuning\n",
    "- Achieves comparable performance with fewer resources\n",
    "\n",
    "### Model Details\n",
    "- **Model**: SmolLM2-135M (135 million parameters)\n",
    "- **Method**: LoRA with r=16 (low rank)\n",
    "- **Task**: Instruction following / Chat completion\n",
    "- **Dataset**: Same 100 samples as Colab 1\n",
    "\n",
    "### Key Difference from Colab 1:\n",
    "- **Colab 1 (Full)**: r=256 â†’ Updates ~36% of parameters (~78M params)\n",
    "- **Colab 2 (LoRA)**: r=16 â†’ Updates ~2% of parameters (~4M params)\n",
    "- **Result**: LoRA is **faster** and uses **less memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c247d",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "We'll install Unsloth and other dependencies needed for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "!pip install -q unsloth\n",
    "\n",
    "# Install additional required packages\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7afa63c",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Import all necessary libraries and disable wandb tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Disable wandb tracking for simplicity\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(\"âœ“ Weights & Biases tracking disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327a515",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model Parameters\n",
    "\n",
    "Same configuration as Colab 1 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 512\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Same model as Colab 1\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"  4-bit Quantization: {load_in_4bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e84771",
   "metadata": {},
   "source": [
    "## Step 4: Load the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f296436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a668f0",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Model for LoRA Fine-tuning\n",
    "\n",
    "**KEY DIFFERENCE**: Using r=16 (LOW rank) for LoRA!\n",
    "\n",
    "### Comparison:\n",
    "- **Colab 1**: r=256 (high rank) â†’ ~78M trainable parameters\n",
    "- **Colab 2**: r=16 (low rank) â†’ ~4M trainable parameters\n",
    "\n",
    "**Benefits of LoRA (r=16)**:\n",
    "- âœ… Much faster training\n",
    "- âœ… Less memory usage\n",
    "- âœ… Smaller model files\n",
    "- âœ… Easy to switch between adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38002dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model with LOW rank for LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LOW rank = Parameter-efficient LoRA!\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,  # Match with r\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model prepared for LoRA fine-tuning!\")\n",
    "print(f\"  Using LOW rank (r=16) for parameter-efficient training\")\n",
    "print(f\"  This updates only ~2% of parameters (much less than Colab 1)\")\n",
    "print(f\"\\nTrainable parameters will be shown when training starts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c468c",
   "metadata": {},
   "source": [
    "## Step 6: Load and Prepare Training Dataset\n",
    "\n",
    "Using the exact same dataset as Colab 1 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2254b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load same dataset as Colab 1\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dataset = dataset.select(range(100))  # Same 100 examples\n",
    "\n",
    "print(f\"âœ“ Dataset loaded: {len(dataset)} examples\")\n",
    "print(\"  (Same dataset as Colab 1 for fair comparison)\")\n",
    "print(\"\\nSample example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208e565",
   "metadata": {},
   "source": [
    "## Step 7: Define Chat Template and Formatting\n",
    "\n",
    "Same Alpaca format as Colab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompt template as Colab 1\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"âœ“ Dataset formatted successfully!\")\n",
    "print(\"\\nFormatted example (first 500 chars):\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdb031",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training Arguments\n",
    "\n",
    "Same training configuration as Colab 1 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same training configuration as Colab 1\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"none\",  # Disable wandb\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training arguments configured!\")\n",
    "print(f\"  Total steps: {training_args.max_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Note: Same config as Colab 1, but with LoRA (r=16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ce4f1",
   "metadata": {},
   "source": [
    "## Step 9: Initialize the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b713d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e73a42",
   "metadata": {},
   "source": [
    "## Step 10: Train the Model with LoRA\n",
    "\n",
    "**Watch**: This should be FASTER than Colab 1 due to fewer trainable parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting LoRA training...\")\n",
    "print(\"This uses r=16 (LOW rank) - much fewer parameters than Colab 1!\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ LoRA Training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"Samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"\\nCompare this time with Colab 1 - LoRA should be faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b868b",
   "metadata": {},
   "source": [
    "## Step 11: Test the LoRA Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60be185",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_instruction = \"Explain what machine learning is in simple terms.\"\n",
    "test_input = \"\"\n",
    "\n",
    "test_prompt = alpaca_prompt.format(test_instruction, test_input, \"\")\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"LoRA Model Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96050eb5",
   "metadata": {},
   "source": [
    "## Step 12: More Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(instruction, input_text=\"\"):\n",
    "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"Input: {input_text}\")\n",
    "    print(f\"\\nResponse:\")\n",
    "    try:\n",
    "        response_part = response.split('### Response:')[1].strip()\n",
    "        print(response_part)\n",
    "    except:\n",
    "        print(response)\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Testing the LoRA fine-tuned model...\\n\")\n",
    "\n",
    "test_model(\"Write a haiku about programming.\")\n",
    "test_model(\"What are the benefits of exercise?\")\n",
    "test_model(\"Summarize this text.\", \"Python is a high-level programming language known for its simplicity and readability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c920f",
   "metadata": {},
   "source": [
    "## Step 13: Save the LoRA Model\n",
    "\n",
    "LoRA adapters are much smaller than full models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f71c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"smollm2_135m_lora_adapters\")\n",
    "tokenizer.save_pretrained(\"smollm2_135m_lora_adapters\")\n",
    "\n",
    "print(\"âœ“ LoRA adapters saved to 'smollm2_135m_lora_adapters' directory\")\n",
    "print(\"\\nNote: LoRA adapter files are MUCH smaller than full model!\")\n",
    "print(\"  - Full model: ~270MB\")\n",
    "print(\"  - LoRA adapters: ~10-20MB\")\n",
    "print(\"\\nYou can load these adapters on top of the base model anytime!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ac66c",
   "metadata": {},
   "source": [
    "## Step 14: Comparison Summary\n",
    "\n",
    "Let's compare LoRA (Colab 2) with Full Fine-tuning (Colab 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: LoRA (Colab 2) vs Full Fine-tuning (Colab 1)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š Parameter Efficiency:\")\n",
    "print(\"  Colab 1 (Full, r=256):  ~78M trainable params (36.75%)\")\n",
    "print(\"  Colab 2 (LoRA, r=16):   ~4M trainable params (~2%)\")\n",
    "print(\"  Difference:             LoRA uses 95% FEWER trainable parameters!\")\n",
    "\n",
    "print(\"\\nâ±ï¸  Training Speed:\")\n",
    "print(f\"  Colab 2 (LoRA):         {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(\"  Expected: LoRA should be faster due to fewer parameters\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Model Size:\")\n",
    "print(\"  Colab 1 (Full):         ~270MB (full model)\")\n",
    "print(\"  Colab 2 (LoRA):         ~10-20MB (adapters only)\")\n",
    "print(\"  Difference:             LoRA is 90% smaller!\")\n",
    "\n",
    "print(\"\\nâœ… Advantages of LoRA:\")\n",
    "print(\"  â€¢ Faster training\")\n",
    "print(\"  â€¢ Less memory usage\")\n",
    "print(\"  â€¢ Smaller model files\")\n",
    "print(\"  â€¢ Can have multiple adapters for different tasks\")\n",
    "print(\"  â€¢ Easy to share and deploy\")\n",
    "\n",
    "print(\"\\nâœ… Advantages of Full Fine-tuning:\")\n",
    "print(\"  â€¢ May achieve slightly better performance\")\n",
    "print(\"  â€¢ More comprehensive parameter updates\")\n",
    "print(\"  â€¢ Better for drastic task changes\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2bb45",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did:\n",
    "1. âœ… Loaded SmolLM2-135M model (same as Colab 1)\n",
    "2. âœ… Configured for **LoRA fine-tuning** (r=16, low rank)\n",
    "3. âœ… Used same dataset (100 examples) for fair comparison\n",
    "4. âœ… Trained for 60 steps (same as Colab 1)\n",
    "5. âœ… Tested the LoRA model\n",
    "6. âœ… Saved LoRA adapters (much smaller!)\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA is parameter-efficient**: Updates only ~2% of parameters\n",
    "- **LoRA is faster**: Fewer parameters = faster training\n",
    "- **LoRA is smaller**: Adapter files are 90% smaller\n",
    "- **LoRA is practical**: Easier to share and deploy\n",
    "- **Performance**: Often comparable to full fine-tuning!\n",
    "\n",
    "### When to Use LoRA vs Full Fine-tuning:\n",
    "- **Use LoRA when**: Limited resources, need speed, multiple tasks\n",
    "- **Use Full when**: Maximum performance needed, have resources\n",
    "\n",
    "### Next Steps:\n",
    "1. âœ… Record video comparing Colab 1 vs Colab 2\n",
    "2. âœ… Highlight the efficiency gains of LoRA\n",
    "3. âž¡ï¸ Move to **Colab 3** for DPO Reinforcement Learning\n",
    "\n",
    "### Resources:\n",
    "- LoRA Paper: https://arxiv.org/abs/2106.09685\n",
    "- Unsloth Documentation: https://docs.unsloth.ai/\n",
    "- LoRA Guide: https://docs.unsloth.ai/get-started/fine-tuning-llms-guide"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
